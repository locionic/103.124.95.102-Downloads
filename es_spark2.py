# -*- coding: utf-8 -*-
"""es-spark2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ifv0YSaEqjeet-6iQKS2XMSk2MADwVDk

# **Set up and install**

## *Set up*
"""
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-11-openjdk-amd64"
os.environ["SPARK_HOME"] = "spark-3.2.0-bin-hadoop2.7"

import findspark
findspark.init()

print(findspark.find())

from pyspark.sql import SparkSession

spark = SparkSession.builder\
        .master("local")\
        .appName("Test")\
        .config("spark.driver.extraClassPath", "elasticsearch-hadoop-7.6.2/dist/elasticsearch-spark-20_2.11-7.6.2.jar") \
        .config("spark.executor.memory", "4g")\
        .config("spark.driver.memory", "4g")\
        .config("spark.cores.max", "2")\
        .getOrCreate()

print(spark)

import json

es_write_conf = { "es.nodes": "localhost", "es.batch.size.entries": "1" ,"es.port" : "9200", "es.resource" : "testindex/testdoc", "es.input.json" : "yes", "es.mapping.id": "doc_id" }

data = [ {'some_key': 'some_value', 'doc_id': 123}, {'some_key': 'some_value', 'doc_id': 456}, {'some_key': 'some_value', 'doc_id': 789} ]
rdd = spark.sparkContext.parallelize(data)

def format_data(x):
    return (x['doc_id'], json.dumps(x))

rdd = rdd.map(lambda x: format_data(x))

# rdd.saveAsNewAPIHadoopFile( \
#     path='-', \
#     outputFormatClass="org.elasticsearch.hadoop.mr.EsOutputFormat", \
#     keyClass="org.apache.hadoop.io.NullWritable", \
#     valueClass="org.elasticsearch.hadoop.mr.LinkedMapWritable", \
#     conf=es_write_conf)

print(rdd.collect())

rdd.saveAsNewAPIHadoopFile( \
    path='-', \
    outputFormatClass="org.elasticsearch.hadoop.mr.EsOutputFormat", \
    keyClass="org.apache.hadoop.io.NullWritable", \
    valueClass="org.elasticsearch.hadoop.mr.LinkedMapWritable", \
    conf=es_write_conf)
    
print("DONE")
